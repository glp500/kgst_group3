{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import cast\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import owlready2\n",
    "import owlrl\n",
    "import requests\n",
    "from conceptnet5 import uri as cn_uri, nodes as cn_nodes, api as cn_api\n",
    "from conceptnet5.nodes import standardized_concept_uri\n",
    "from pydantic import BaseModel\n",
    "from rdflib import Graph, URIRef, DCTERMS, RDFS, Literal, Namespace, RDF, OWL, SKOS, BNode\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Whether to check if a DBPedia link exists for a given concept\n",
    "DONT_CHECK_FOR_URI_EXISTS = False"
   ],
   "id": "2d3d5158d53a7664",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the output from the research paper information extraction pipeline\n",
    "file = Path(os.getcwd()).joinpath(\"final_concepts.csv\")\n",
    "results_file = open(file)\n",
    "results_file_contents: str = results_file.read()"
   ],
   "id": "31b3a4bb8fa560c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "paper_dois_and_titles = [\n",
    "    [\"10.3233/FAIA240179\", \"https://doi.org/10.3233/FAIA240179\",\n",
    "     \"Formal Specification of Actual Trust in Multiagent Systems\"],\n",
    "    [\"10.3233/FAIA240201\", \"https://doi.org/10.3233/FAIA240201\",\n",
    "     \"Common Ground Provides a Mental Shortcut in Agent-Agent Interaction\"],\n",
    "    [\"hhai-2021_paper_46.pdf\",\n",
    "     \"https://www.hhai-conference.org/wp-content/uploads/2022/08/hhai-2021_paper_46.pdf\",\n",
    "     \"Training Intelligent Tutors on User Simulators Using Reinforcement Learning\"],\n",
    "    [\"10.3233/FAIA230099\", \"https://doi.org/10.3233/FAIA230099\",\n",
    "     \"Towards Robots at Meet Users’ Need for Explanation\"],\n",
    "    [\"10.3233/FAIA240187\", \"https://doi.org/10.3233/FAIA240187\",\n",
    "     \"EASY-AI: sEmantic And compoSable glYphs for representing AI systems\"],\n",
    "    [\"10.3233/FAIA230097\", \"https://doi.org/10.3233/FAIA230097\",\n",
    "     \"Co-Performing Music with AI: Real-Time Performance Control Using Speech and Gestures\"],\n",
    "    [\"10.3233/FAIA230090\", \"https://doi.org/10.3233/FAIA230090\",\n",
    "     \"Design of a Human-in-the-Loop Centered AI-Based Clinical Decision Support System for Professional Care Planning\"],\n",
    "    [\"10.3233/FAIA230073\", \"https://doi.org/10.3233/FAIA230073\",\n",
    "     \"Human Factors in Interactive Online Machine Learning\"],\n",
    "    [\"10.3233/FAIA220187\", \"https://doi.org/10.3233/FAIA220187\",\n",
    "     \"HyEnA: A Hybrid Method for Extracting Arguments from Opinions\"],\n",
    "    [\"10.3233/FAIA230080\", \"https://doi.org/10.3233/FAIA230080\",\n",
    "     \"Visualizing Deep Neural Networks with Topographic Activation Maps\"],\n",
    "    [\"arxiv/2303.00866\", \"https://arxiv.org/abs/2303.00866\",\n",
    "     \"A prototype hybrid prediction market for estimating replicability of published work\"],\n",
    "]"
   ],
   "id": "134ae55eb56f8fd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lines = results_file_contents.split(\"\\n\")\n",
    "result_lines = lines[1:]\n",
    "print(lines[0])  # Header"
   ],
   "id": "f6fab7adebcde94d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(result_lines[1])",
   "id": "5285ca5e9856f4cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Classes used to model the results\n",
    "class Scenario(BaseModel):\n",
    "    main: str\n",
    "    description: str\n",
    "    contains: list[str]\n",
    "\n",
    "\n",
    "class Task(BaseModel):\n",
    "    main: str\n",
    "    description: str\n",
    "    hasActors: list[str]\n",
    "    usedIn: list[str]\n",
    "\n",
    "\n",
    "class IndividualActor(BaseModel):\n",
    "    name: str\n",
    "    type: str\n",
    "    has: list[str]\n",
    "\n",
    "\n",
    "class Actor(BaseModel):\n",
    "    actors: list[IndividualActor]\n",
    "\n",
    "\n",
    "class IndividualInformationProcessing(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    hasProcessingMethod: list[str]\n",
    "    produces: list[str]\n",
    "\n",
    "\n",
    "class InformationProcessing(BaseModel):\n",
    "    information_processing: list[IndividualInformationProcessing]\n",
    "\n",
    "\n",
    "class IndividualProcessingMethod(BaseModel):\n",
    "    name: str\n",
    "    type: str\n",
    "\n",
    "\n",
    "class ProcessingMethod(BaseModel):\n",
    "    methods: list[IndividualProcessingMethod]\n",
    "\n",
    "\n",
    "class IndividualMetric(BaseModel):\n",
    "    name: str\n",
    "    type: str\n",
    "\n",
    "\n",
    "class Metric(BaseModel):\n",
    "    metrics: list[IndividualMetric]\n",
    "\n",
    "\n",
    "class Result(BaseModel):\n",
    "    paper_id: str\n",
    "    paper_url: str\n",
    "    paper_title: str\n",
    "    scenario: Scenario\n",
    "    task: Task\n",
    "    actor: Actor\n",
    "    information_processing: InformationProcessing\n",
    "    processing_method: ProcessingMethod\n",
    "    metric: Metric\n",
    "\n",
    "\n",
    "def process_csv_part(line):\n",
    "    line = line.replace(\"\\\"\\\"\", \"\\\"\")\n",
    "\n",
    "    line = ('{ \"main\": ' + line + '}').replace(\"\\\"\\\"\", \"\\\"\")\n",
    "\n",
    "    line_json = json.loads(line)\n",
    "    return line_json\n",
    "\n",
    "\n",
    "def process_csv_actor_part(line):\n",
    "    line = line.replace(\"\\\"\\\"\", \"\\\"\")\n",
    "\n",
    "    line = ('{\"actors\": [ { \"name\": ' + line + '}]}').replace(\"\\\"\\\"\", \"\\\"\")\n",
    "\n",
    "    line_json = json.loads(line)\n",
    "\n",
    "    return line_json\n",
    "\n",
    "\n",
    "def process_csv_information_processing_part(line):\n",
    "    line = line.replace(\"\\\"\\\"\", \"\\\"\")\n",
    "\n",
    "    line = ('{\"information_processing\": [ { \"name\": ' + line + '}]}').replace(\"\\\"\\\"\", \"\\\"\")\n",
    "\n",
    "    line_json = json.loads(line)\n",
    "    return line_json\n",
    "\n",
    "\n",
    "def process_csv_processing_method_part(line):\n",
    "    line = line.replace(\"\\\"\\\"\", \"\\\"\")\n",
    "\n",
    "    line = ('{\"methods\": [ { \"name\": ' + line + '}]}').replace(\"\\\"\\\"\", \"\\\"\")\n",
    "\n",
    "    line_json = json.loads(line)\n",
    "    return line_json\n",
    "\n",
    "\n",
    "def process_csv_metric_part(line):\n",
    "    line = line.replace(\"\\\"\\\"\", \"\\\"\")\n",
    "\n",
    "    line = ('{\"metrics\": [ { \"name\": ' + line + '}]}').replace(\"\\\"\\\"\", \"\\\"\")\n",
    "\n",
    "    line_json = json.loads(line)\n",
    "    return line_json\n",
    "\n",
    "\n",
    "results: list[Result] = []\n",
    "for i, line in enumerate(result_lines):\n",
    "    scenario_and_task_regex_string = open(\n",
    "        Path(os.getcwd()).joinpath(\"scenario_and_task_regex.txt\")).read()\n",
    "    scenario_and_task_regex = re.compile(scenario_and_task_regex_string)\n",
    "    scenario_and_task = scenario_and_task_regex.findall(line)\n",
    "\n",
    "    scenario = process_csv_part(scenario_and_task[0])\n",
    "    task = process_csv_part(scenario_and_task[1])\n",
    "\n",
    "    rest_regex_string = open(\n",
    "        Path(os.getcwd()).joinpath(\"rest_regex.txt\")).read()\n",
    "    rest_regex = re.compile(rest_regex_string)\n",
    "    rest = rest_regex.findall(line)\n",
    "\n",
    "    actor = process_csv_actor_part(rest[0])\n",
    "    information_processing = process_csv_information_processing_part(rest[1])\n",
    "    processing_method = process_csv_processing_method_part(rest[2])\n",
    "    metric = process_csv_metric_part(rest[3])\n",
    "\n",
    "    result = {\n",
    "        \"paper_id\": paper_dois_and_titles[i][0],\n",
    "        \"paper_url\": paper_dois_and_titles[i][1],\n",
    "        \"paper_title\": paper_dois_and_titles[i][2],\n",
    "        \"scenario\": scenario,\n",
    "        \"task\": task,\n",
    "        \"actor\": actor,\n",
    "        \"information_processing\": information_processing,\n",
    "        \"processing_method\": processing_method,\n",
    "        \"metric\": metric\n",
    "    }\n",
    "\n",
    "    result = Result.model_validate(result)\n",
    "    results.append(result)\n"
   ],
   "id": "6cc6cc40905d8dc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for result in results:\n",
    "    print(result.model_dump_json(indent=4))"
   ],
   "id": "f9729a45f9793a93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Conceptnet word embeddings for finding (out-of-vocabulary) concepts\n",
    "vectors = {}\n",
    "with open(Path(os.getcwd()).joinpath(\"./numberbatch-en.txt\"), encoding='utf-8') as f:\n",
    "    next(f)  # skip header\n",
    "    for i, line in enumerate(f):\n",
    "        parts = line.strip().split()\n",
    "        word = parts[0]\n",
    "        vec = np.array(list(map(float, parts[1:])))\n",
    "        vectors[word] = vec\n",
    "        # if i >= 1000:\n",
    "        #     break"
   ],
   "id": "6c7e9949a636a12a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ],
   "id": "2e1aa8315371b70f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_conceptnet_words(vectors_dict):\n",
    "    return [\n",
    "        c.replace('_', ' ')\n",
    "        for c in vectors_dict.keys()\n",
    "        if c.startswith('/c/en/') or not c.startswith('/')\n",
    "    ]\n",
    "\n",
    "\n",
    "# Load ConceptNet entries\n",
    "concepts = load_conceptnet_words(vectors)\n",
    "\n",
    "# Embed concept names with the same model\n",
    "concept_vecs = model.encode(concepts)"
   ],
   "id": "69f9a3a95d7564",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "CONCEPTNET_NAMESPACE = Namespace(\"https://api.conceptnet.io/c/en/\")\n",
    "DBPEDIA_NAMESPACE = Namespace(\"http://dbpedia.org/resource/\")\n",
    "\n",
    "concept_vecs = np.array(concept_vecs).astype('float32')\n",
    "faiss.normalize_L2(concept_vecs)\n",
    "\n",
    "\n",
    "def find_nearest_concepts(query: str):\n",
    "    global concept_vecs\n",
    "\n",
    "    query_vec = model.encode(query).reshape(1, -1)\n",
    "\n",
    "    query_vec = query_vec.astype('float32')\n",
    "\n",
    "    # Normalize for cosine similarity\n",
    "    faiss.normalize_L2(query_vec)\n",
    "\n",
    "    # Use FAISS due to the size of the dictionary\n",
    "    index = faiss.IndexFlatIP(concept_vecs.shape[1])\n",
    "    index.add(concept_vecs)\n",
    "\n",
    "    k = 5\n",
    "    scores, indices = index.search(query_vec, k)\n",
    "\n",
    "    results: list[dict[\"concept\" or \"score\", str]] = []\n",
    "    for idx, score in zip(indices[0], scores[0]):\n",
    "        if score > 0.8:\n",
    "            results.append({\n",
    "                \"concept\": URIRef(standardized_concept_uri(\"en\", concepts[idx]),\n",
    "                                  base=CONCEPTNET_NAMESPACE),\n",
    "                \"score\": score\n",
    "            })\n",
    "            print(f\"{word} ==>  {concepts[idx]} — {score:.4f}\")\n",
    "\n",
    "    return results"
   ],
   "id": "a1ce64988cbe2842",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Uses reification to be able to add metadata to the relationships\n",
    "def reificate_information_processing_for_actor(individual_actor_ref: URIRef | BNode,\n",
    "                                               individual_information_processing_ref: URIRef) -> URIRef:\n",
    "    reificated_information_processing_for_actor_ref = URIRef(\n",
    "        individual_actor_ref + \"_processing_information_\" + individual_information_processing_ref,\n",
    "        base=paper_ref)\n",
    "    my_graph.add((\n",
    "        reificated_information_processing_for_actor_ref,\n",
    "        RDF.type,\n",
    "        RDF.Statement\n",
    "    ))\n",
    "    my_graph.add((\n",
    "        reificated_information_processing_for_actor_ref,\n",
    "        RDF.subject,\n",
    "        URIRef(individual_actor_ref)\n",
    "    ))\n",
    "    my_graph.add((\n",
    "        reificated_information_processing_for_actor_ref,\n",
    "        RDF.predicate,\n",
    "        hi_processingInformationProperty_ref\n",
    "    ))\n",
    "    my_graph.add((\n",
    "        reificated_information_processing_for_actor_ref,\n",
    "        RDF.object,\n",
    "        individual_information_processing_ref\n",
    "    ))\n",
    "\n",
    "    return reificated_information_processing_for_actor_ref\n",
    "\n",
    "\n",
    "def reificate_interaction_for_scenario(individual_interaction_ref: URIRef,\n",
    "                                       individual_scenario_ref: URIRef | BNode) -> URIRef:\n",
    "    reficiated_interaction_for_scenario_ref = URIRef(\n",
    "        individual_scenario_ref + \"_interaction_\" + individual_interaction_ref, base=paper_ref)\n",
    "    my_graph.add((\n",
    "        reficiated_interaction_for_scenario_ref,\n",
    "        RDF.type,\n",
    "        RDF.Statement\n",
    "    ))\n",
    "    my_graph.add((\n",
    "        reficiated_interaction_for_scenario_ref,\n",
    "        RDF.subject,\n",
    "        URIRef(individual_scenario_ref)\n",
    "    ))\n",
    "    my_graph.add((\n",
    "        reficiated_interaction_for_scenario_ref,\n",
    "        RDF.predicate,\n",
    "        hi_hasInteractionProperty_ref\n",
    "    ))\n",
    "    my_graph.add((\n",
    "        reficiated_interaction_for_scenario_ref,\n",
    "        RDF.object,\n",
    "        individual_interaction_ref\n",
    "    ))\n",
    "\n",
    "    return reficiated_interaction_for_scenario_ref\n",
    "\n",
    "\n",
    "def random_delay():\n",
    "    delay = random.uniform(0, 0.2)  # random delay between 1 and 3 seconds\n",
    "    print(f\"Sleeping for {delay:.2f} seconds\")\n",
    "    time.sleep(delay)\n",
    "\n",
    "\n",
    "def dbpedia_exists(uri: str):\n",
    "    print(DONT_CHECK_FOR_URI_EXISTS)\n",
    "    if DONT_CHECK_FOR_URI_EXISTS:\n",
    "        return True\n",
    "\n",
    "    random_delay()\n",
    "\n",
    "    response = requests.head(uri, allow_redirects=True)\n",
    "    return response.status_code == 200\n",
    "\n",
    "\n",
    "def add_conceptnet_keywords(word: str, related_concepts_added_to_ref: URIRef):\n",
    "    nearest_concepts = find_nearest_concepts(word)\n",
    "\n",
    "    refs: list[URIRef] = []\n",
    "    for concept in nearest_concepts:\n",
    "        concept_name = concept[\"concept\"].split(\"/\")[-1].replace(\" \", \"_\").capitalize()\n",
    "\n",
    "        refs.append(concept[\"concept\"])\n",
    "        my_graph.add((\n",
    "            related_concepts_added_to_ref,\n",
    "            DCTERMS.subject,\n",
    "            concept[\"concept\"]\n",
    "        ))\n",
    "\n",
    "        dbpedia_link = URIRef(\n",
    "            concept_name,\n",
    "            base=DBPEDIA_NAMESPACE\n",
    "        )\n",
    "        print(dbpedia_link)\n",
    "        if dbpedia_exists(dbpedia_link):\n",
    "            my_graph.add((\n",
    "                related_concepts_added_to_ref,\n",
    "                DCTERMS.subject,\n",
    "                dbpedia_link\n",
    "            ))\n",
    "\n",
    "        else:\n",
    "            print(f\"{dbpedia_link} does not exist\")\n",
    "\n",
    "    return refs\n",
    "\n",
    "\n",
    "# Populate graph with instances\n",
    "my_graph = Graph()\n",
    "my_graph.remove((None, None, None))\n",
    "my_graph.parse(\"./hi_ontology.ttl\")\n",
    "\n",
    "KGST_NAMESPACE = Namespace(\n",
    "    \"https://github.com/EliasLiinamaa/kgst_project_group_3/final_ontology_extension.ttl#\")\n",
    "my_graph.namespace_manager.bind(\"\", KGST_NAMESPACE)\n",
    "\n",
    "BIBO_NAMESPACE = Namespace(\"http://purl.org/ontology/bibo/\")\n",
    "my_graph.namespace_manager.bind(\"bibo\", BIBO_NAMESPACE)\n",
    "\n",
    "HI_NAMESPACE = Namespace(\n",
    "    \"http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/\")\n",
    "my_graph.namespace_manager.bind(\"hi\", HI_NAMESPACE)\n",
    "\n",
    "my_graph.namespace_manager.bind(\"conceptnet\", CONCEPTNET_NAMESPACE)\n",
    "my_graph.namespace_manager.bind(\"dbr\", DBPEDIA_NAMESPACE)\n",
    "\n",
    "bibo_AcademicArticleClass_ref = URIRef(\"AcademicArticle\", base=BIBO_NAMESPACE)\n",
    "kgst_HIAcademicArticleClass_ref = URIRef(\"HybridIntelligenceAcademicArticle\", base=KGST_NAMESPACE)\n",
    "\n",
    "kgst_examinesScenarioProperty_ref = URIRef(\"examinesScenario\", base=KGST_NAMESPACE)\n",
    "kgst_scenarioInProperty_ref = URIRef(\"scenarioIn\", base=KGST_NAMESPACE)\n",
    "\n",
    "hi_ScenarioClass_ref = URIRef(\"Scenario\", base=HI_NAMESPACE)\n",
    "hi_hasInteractionProperty_ref = URIRef(\"hasInteraction\", base=HI_NAMESPACE)\n",
    "\n",
    "hi_InteractionClass_ref = URIRef(\"Interaction\", base=HI_NAMESPACE)\n",
    "kgst_usesProcessProperty_ref = URIRef(\"usesProcess\", base=KGST_NAMESPACE)\n",
    "hi_interactingAgentProperty_ref = URIRef(\"interactingAgent\", base=HI_NAMESPACE)\n",
    "\n",
    "hi_ActorClass_ref = URIRef(\"Actor\", base=HI_NAMESPACE)\n",
    "hi_HumanClass_ref = URIRef(\"Human\", base=HI_NAMESPACE)\n",
    "hi_ArtificialAgentClass_ref = URIRef(\"ArtificialAgent\", base=HI_NAMESPACE)\n",
    "hi_inScenarioProperty_ref = URIRef(\"inScenario\", base=HI_NAMESPACE)\n",
    "\n",
    "hi_processingInformationProperty_ref = URIRef(\"processingInformation\", base=HI_NAMESPACE)\n",
    "\n",
    "hi_InformationProcessingClass_ref = URIRef(\"InformationProcessing\", base=HI_NAMESPACE)\n",
    "hi_informationMethodProperty_ref = URIRef(\"informationMethod\", base=HI_NAMESPACE)\n",
    "\n",
    "hi_ProcessingMethodClass_ref = URIRef(\"ProcessingMethod\", base=HI_NAMESPACE)\n",
    "# Under which dynamically generated sybclasses like `kgst:Symbolic` or `kgst:Statistical` etc.\n",
    "\n",
    "kgst_metricProperty_ref = URIRef(\"metric\", base=KGST_NAMESPACE)\n",
    "kgst_MetricClass_ref = URIRef(\"Metric\", base=KGST_NAMESPACE)\n",
    "kgst_QuantitativeMetric_ref = URIRef(\"QuantitativeMetric\", base=KGST_NAMESPACE)\n",
    "kgst_QualitativeMetric_ref = URIRef(\"QualitativeMetric\", base=KGST_NAMESPACE)\n",
    "\n",
    "# ====================\n",
    "# Research papers\n",
    "my_graph.add((\n",
    "    kgst_HIAcademicArticleClass_ref,\n",
    "    RDFS.subClassOf,\n",
    "    bibo_AcademicArticleClass_ref\n",
    "))\n",
    "\n",
    "my_graph.add((\n",
    "    kgst_examinesScenarioProperty_ref,\n",
    "    RDFS.range,\n",
    "    hi_ScenarioClass_ref\n",
    "))\n",
    "\n",
    "# ====================\n",
    "# Scenarios\n",
    "my_graph.add((\n",
    "    kgst_scenarioInProperty_ref,\n",
    "    OWL.inverseOf,\n",
    "    kgst_examinesScenarioProperty_ref\n",
    "))\n",
    "\n",
    "# ====================\n",
    "# Interactions\n",
    "my_graph.add((\n",
    "    kgst_usesProcessProperty_ref,\n",
    "    RDFS.domain,\n",
    "    hi_InteractionClass_ref\n",
    "))\n",
    "my_graph.add((\n",
    "    kgst_usesProcessProperty_ref,\n",
    "    RDFS.range,\n",
    "    hi_InformationProcessingClass_ref\n",
    "))\n",
    "\n",
    "# ====================\n",
    "# Metrics\n",
    "my_graph.add((\n",
    "    kgst_QuantitativeMetric_ref,\n",
    "    RDFS.subClassOf,\n",
    "    kgst_MetricClass_ref\n",
    "))\n",
    "my_graph.add((\n",
    "    kgst_QualitativeMetric_ref,\n",
    "    RDFS.subClassOf,\n",
    "    kgst_MetricClass_ref\n",
    "))\n",
    "my_graph.add((\n",
    "    kgst_metricProperty_ref,\n",
    "    RDFS.range,\n",
    "    kgst_MetricClass_ref\n",
    "))\n",
    "\n",
    "# Go through each paper\n",
    "for paper in results:\n",
    "    paper_ref = URIRef(paper.paper_id, base=KGST_NAMESPACE)\n",
    "\n",
    "    # Add paper to the graph\n",
    "    my_graph.add((\n",
    "        paper_ref,\n",
    "        RDF.type,\n",
    "        kgst_HIAcademicArticleClass_ref\n",
    "    ))\n",
    "\n",
    "    # Basic details\n",
    "    my_graph.add((\n",
    "        paper_ref,\n",
    "        DCTERMS.title,\n",
    "        Literal(paper.paper_title)\n",
    "    ))\n",
    "    my_graph.add((\n",
    "        paper_ref,\n",
    "        RDFS.label,\n",
    "        Literal(paper.paper_title)\n",
    "    ))\n",
    "    my_graph.add((\n",
    "        paper_ref,\n",
    "        OWL.sameAs,\n",
    "        URIRef(paper.paper_url)\n",
    "    ))\n",
    "\n",
    "    # Scenario\n",
    "    individual_scenario_ref = URIRef(f\"_scenario_{paper.scenario.main.replace(' ', '_')}\",\n",
    "                                     base=paper_ref)\n",
    "    my_graph.add((\n",
    "        individual_scenario_ref,\n",
    "        RDF.type,\n",
    "        hi_ScenarioClass_ref\n",
    "    ))\n",
    "    my_graph.add((\n",
    "        paper_ref,\n",
    "        kgst_examinesScenarioProperty_ref,\n",
    "        individual_scenario_ref\n",
    "    ))\n",
    "    my_graph.add((\n",
    "        individual_scenario_ref,\n",
    "        RDFS.label,\n",
    "        Literal(paper.scenario.main)\n",
    "    ))\n",
    "    my_graph.add((\n",
    "        individual_scenario_ref,\n",
    "        DCTERMS.description,\n",
    "        Literal(paper.scenario.description)\n",
    "    ))\n",
    "    add_conceptnet_keywords(paper.scenario.main, individual_scenario_ref)\n",
    "\n",
    "    # Add interactions\n",
    "    specific_interactions: dict[URIRef, URIRef] = {}\n",
    "    for interaction in paper.scenario.contains:\n",
    "        individual_interaction_ref = URIRef(\n",
    "            interaction.replace(\" \", \"_\"), base=KGST_NAMESPACE)\n",
    "        my_graph.add((\n",
    "            individual_interaction_ref,\n",
    "            RDFS.label,\n",
    "            Literal(interaction)\n",
    "        ))\n",
    "\n",
    "        reificated_interaction_for_scenario_ref = reificate_interaction_for_scenario(\n",
    "            individual_interaction_ref, individual_scenario_ref)\n",
    "        specific_interactions[individual_interaction_ref] = reificated_interaction_for_scenario_ref\n",
    "\n",
    "        add_conceptnet_keywords(interaction, individual_interaction_ref)\n",
    "\n",
    "    # Task that may or may not have already been included in paper.scenario.contains\n",
    "    individual_interaction_ref = URIRef(\n",
    "        paper.task.main.replace(\" \", \"_\"), base=KGST_NAMESPACE)\n",
    "\n",
    "    reificated_interaction_for_scenario_ref: URIRef\n",
    "    if individual_interaction_ref in specific_interactions:\n",
    "        reificated_interaction_for_scenario_ref = specific_interactions[\n",
    "            individual_interaction_ref]\n",
    "    else:\n",
    "        reificated_interaction_for_scenario_ref = reificate_interaction_for_scenario(\n",
    "            individual_interaction_ref, individual_scenario_ref)\n",
    "\n",
    "    my_graph.add((\n",
    "        reificated_interaction_for_scenario_ref,\n",
    "        RDFS.label,\n",
    "        Literal(paper.task.main)\n",
    "    ))\n",
    "\n",
    "    # Add actors\n",
    "    for actor in paper.task.hasActors:\n",
    "        individual_actor_ref = URIRef(\"_actor_\" + actor.replace(\" \", \"_\"),\n",
    "                                      base=paper_ref)\n",
    "        my_graph.add((\n",
    "            reificated_interaction_for_scenario_ref,\n",
    "            hi_interactingAgentProperty_ref,\n",
    "            individual_actor_ref\n",
    "        ))\n",
    "\n",
    "        add_conceptnet_keywords(actor, individual_actor_ref)\n",
    "\n",
    "    # Add information processing\n",
    "    for process in paper.task.usedIn:\n",
    "        individual_information_processing_ref = URIRef(\n",
    "            process.replace(\" \", \"_\"),\n",
    "            base=KGST_NAMESPACE)\n",
    "        my_graph.add((\n",
    "            reificated_interaction_for_scenario_ref,\n",
    "            kgst_usesProcessProperty_ref,\n",
    "            individual_information_processing_ref\n",
    "        ))\n",
    "        my_graph.add((\n",
    "            individual_information_processing_ref,\n",
    "            RDF.type,\n",
    "            hi_InformationProcessingClass_ref\n",
    "        ))\n",
    "        add_conceptnet_keywords(process, individual_information_processing_ref)\n",
    "\n",
    "    # Process additional details for actors\n",
    "    actor_information_processings: dict[\n",
    "        URIRef, URIRef] = cast(dict[URIRef, URIRef],\n",
    "                               {})  # \"individual_information_processing_uri_ref\" : \"reificated_information_processing_for_actor_ref\"\n",
    "    for actor in paper.actor.actors:\n",
    "        individual_actor_ref = URIRef(\"_actor_\" + actor.name.replace(\" \", \"_\"),\n",
    "                                      base=paper_ref)\n",
    "\n",
    "        add_conceptnet_keywords(actor.name, individual_actor_ref)\n",
    "\n",
    "        my_graph.add((\n",
    "            individual_actor_ref,\n",
    "            hi_inScenarioProperty_ref,\n",
    "            individual_scenario_ref,\n",
    "        ))\n",
    "\n",
    "        assert actor.type in [\"Human\", \"Artificial Agent\"]\n",
    "        my_graph.add((\n",
    "            individual_actor_ref,\n",
    "            RDF.type,\n",
    "            hi_HumanClass_ref if actor.type.lower() == \"human\" else hi_ArtificialAgentClass_ref\n",
    "        ))  # Shouldn't set range of schema participant\n",
    "\n",
    "        # Add actor's information processing\n",
    "        for actor_information_processing in actor.has:\n",
    "            individual_information_processing_uri_ref = URIRef(\n",
    "                actor_information_processing.replace(\" \", \"_\"), base=KGST_NAMESPACE)\n",
    "            reificated_information_processing_for_actor_ref: URIRef = reificate_information_processing_for_actor(\n",
    "                individual_actor_ref,\n",
    "                individual_information_processing_uri_ref)\n",
    "\n",
    "            actor_information_processings[\n",
    "                individual_information_processing_uri_ref] = reificated_information_processing_for_actor_ref\n",
    "\n",
    "        add_conceptnet_keywords(actor.name, individual_actor_ref)\n",
    "\n",
    "    # Process additional details for information processing\n",
    "    metrics: list[URIRef] = []\n",
    "    for information_processing in paper.information_processing.information_processing:\n",
    "        individual_information_processing_ref = URIRef(\n",
    "            information_processing.name.replace(\" \", \"_\"),\n",
    "            base=KGST_NAMESPACE)\n",
    "        my_graph.add((\n",
    "            individual_information_processing_ref,\n",
    "            SKOS.definition,  # \"A formal, human-readable definition of a skos:Concept\"\n",
    "            Literal(information_processing.description)\n",
    "        ))\n",
    "\n",
    "        add_conceptnet_keywords(information_processing.name,\n",
    "                                individual_information_processing_ref)\n",
    "\n",
    "        my_graph.add((\n",
    "            individual_information_processing_ref,\n",
    "            RDFS.label,\n",
    "            Literal(information_processing.name)\n",
    "        ))\n",
    "\n",
    "        reificated_information_processing_for_actor_ref: URIRef\n",
    "        if individual_information_processing_ref in actor_information_processings.keys():\n",
    "            reificated_information_processing_for_actor_ref = actor_information_processings[\n",
    "                individual_information_processing_ref]\n",
    "        else:\n",
    "            print(individual_information_processing_ref, \"has NOT been assigned to an actor\")\n",
    "            reificated_information_processing_for_actor_ref = reificate_information_processing_for_actor(\n",
    "                BNode(),\n",
    "                individual_information_processing_ref)\n",
    "\n",
    "        # Add processing methods for the information processing\n",
    "        for processing_method in information_processing.hasProcessingMethod:\n",
    "            individual_processing_method_ref = URIRef(processing_method.replace(\" \", \"_\"),\n",
    "                                                      base=KGST_NAMESPACE)\n",
    "            my_graph.add((\n",
    "                reificated_information_processing_for_actor_ref,\n",
    "                hi_informationMethodProperty_ref,\n",
    "                individual_processing_method_ref\n",
    "            ))\n",
    "            my_graph.add((\n",
    "                individual_processing_method_ref,\n",
    "                RDF.type,\n",
    "                hi_ProcessingMethodClass_ref\n",
    "            ))\n",
    "            my_graph.add((\n",
    "                individual_processing_method_ref,\n",
    "                RDFS.label,\n",
    "                Literal(processing_method)\n",
    "            ))\n",
    "            add_conceptnet_keywords(processing_method,\n",
    "                                    individual_processing_method_ref)\n",
    "\n",
    "        # Add metrics for the information processing\n",
    "        for metric in information_processing.produces:\n",
    "            individual_metric_ref = URIRef(metric.replace(\" \", \"_\"), base=KGST_NAMESPACE)\n",
    "            my_graph.add((\n",
    "                reificated_information_processing_for_actor_ref,\n",
    "                kgst_metricProperty_ref,\n",
    "                individual_metric_ref\n",
    "            ))\n",
    "            my_graph.add((\n",
    "                individual_metric_ref,\n",
    "                RDF.type,\n",
    "                kgst_MetricClass_ref\n",
    "            ))\n",
    "\n",
    "        add_conceptnet_keywords(information_processing.name,\n",
    "                                individual_information_processing_ref)\n",
    "\n",
    "    # Process additional details for processing methods\n",
    "    for processing_method in paper.processing_method.methods:\n",
    "        individual_processing_method_class_ref = URIRef(\n",
    "            processing_method.type.replace(\" \", \"_\"), base=KGST_NAMESPACE)\n",
    "        my_graph.add((\n",
    "            individual_processing_method_class_ref,\n",
    "            RDFS.subClassOf,\n",
    "            hi_ProcessingMethodClass_ref\n",
    "        ))\n",
    "\n",
    "        individual_processing_method_ref = URIRef(processing_method.name.replace(\" \", \"_\"),\n",
    "                                                  base=KGST_NAMESPACE)\n",
    "        my_graph.add((\n",
    "            individual_processing_method_ref,\n",
    "            RDF.type,\n",
    "            individual_processing_method_class_ref\n",
    "        ))\n",
    "        my_graph.add((\n",
    "            individual_processing_method_ref,\n",
    "            RDFS.label,\n",
    "            Literal(processing_method.name)\n",
    "        ))\n",
    "\n",
    "        add_conceptnet_keywords(processing_method.name, individual_processing_method_ref)\n",
    "\n",
    "    # Process additional details for metrics\n",
    "    for metric in paper.metric.metrics:\n",
    "        individual_metric_ref = URIRef(metric.name.replace(\" \", \"_\"), base=KGST_NAMESPACE)\n",
    "        my_graph.add((\n",
    "            individual_metric_ref,\n",
    "            RDFS.label,\n",
    "            Literal(metric.name)\n",
    "        ))\n",
    "        assert metric.type in [\"Qualitative\", \"Quantitative\"]\n",
    "        my_graph.add((\n",
    "            individual_metric_ref,\n",
    "            RDF.type,\n",
    "            kgst_QualitativeMetric_ref if metric.type.lower() == \"qualitative\" else kgst_QuantitativeMetric_ref\n",
    "        ))\n",
    "        add_conceptnet_keywords(metric.name, individual_metric_ref)"
   ],
   "id": "c188de3e538948f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(my_graph.serialize(format=\"turtle\"))\n",
    "my_graph.serialize(\"./final_ontology_extension.ttl\", format=\"turtle\")\n",
    "my_graph.serialize(\"./final_ontology_extension.nt\", encoding=\"utf-8\", format=\"ntriples\")"
   ],
   "id": "a1a24c6374ebd7ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(len(my_graph))\n",
    "owlrl.DeductiveClosure(owlrl.OWLRL_Semantics).expand(my_graph)\n",
    "print(len(my_graph))\n",
    "my_graph.serialize(\"./final_ontology_extension_inferred.ttl\", format=\"turtle\")"
   ],
   "id": "2f0c464f17bf771b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check for consistency\n",
    "\n",
    "world = owlready2.World()\n",
    "world.get_ontology(\"./final_ontology_extension.nt\", ).load()\n",
    "\n",
    "owlready2.sync_reasoner()\n",
    "\n",
    "inconsistent_classes = list(world.inconsistent_classes())\n",
    "\n",
    "if inconsistent_classes:\n",
    "    print(\"Inconsistent classes found:\")\n",
    "    for cls in inconsistent_classes:\n",
    "        print(cls)\n",
    "else:\n",
    "    print(\"Ontology is consistent!\")"
   ],
   "id": "5d81913444df9604",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
